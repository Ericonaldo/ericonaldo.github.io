---
title: 'Some comprehension to RL'
date: 2019-09-01
permalink: /posts/2019/09/blog-post/
tags:
  - reinforcement-learning
  - comprehension-points
---
2018 09-01 17:25

到底什么是on/off policy?
----------------------

采样和更新是同一策略即代表on policy，并不需要每一步都更新，只要更新用的采样得到的数据是待更新策略采到的就OK. Off policy例如dqn、ddpg的\epsilon-greedy（采样）和greedy策略（被更新）。因此后者的数据利用率更高，因为之前采到的数据不需要扔。而on policy则需要边采边扔掉过去策略的数据（因为梯度不对了）。

为什么在采用replay buffer时需要importance sampling？
------------------------------------------------

为什么V需要？考虑V的定义是在策略pi下每个状态的价值函数，因此不同的策略需要进行重要性采样，即在某状态选择同一动作的概率之比。因此不能用MC方法求得回报，因为每一步重要性采样累乘的积会变得很小，导致方差很大。因此适合TD learning，只需要一步的动作概率。

但是注意在TD learning时Q不需要重要性采样（见q学习），因为Q(s,a)更新时用到的successor的动作是由当前策略给出，因此不需要。

什么是Actor-Critic？和PG的关系？
-----------------------------

AC可以看做一般的AC方法或者一种框架。是一类policy gradient的方法。其实我认为**最初的AC和REINFORCE(Stochastic Policy Gradient)应该是等价的**，不同的是在计算梯度的时候，后者采用Monte Carlo的方式计算梯度中的价值项（本质也可看做是一个critic）（累加计算轨迹回报），而AC是利用TD实现（借助value估计，因此也有说是结合了value based方法和policy based方法）在线更新！**其实只要不是计算梯度时的价值项是用的轨迹累积奖励G，只要涉及到价值估计都是AC。**AC本身是一种PG方法，同时又结合了value estimation方法，所以有些地方将之归为PG方法的一种，有些地方把它列为policy-based和value-based以外的另一种方法。

DDPG也是基于AC框架但利用确定性策略，结合了DQN的成功经验，可以提高样本利用率（off policy）。

而PPO和TRPO也结合了AC框架，但是走了另一条优化道路，利用优化方法中的信頼域方法，用到了自然梯度，选择了代理目标（surrogate target）进行策略优化（保证策略单调改进的迭代方法）。PPO是TRPO的一阶近似。后来PPO的改进版本对替代目标函数进行了进一步改进，使得算法更加简单，效果更好。

Deepmind又搞了类似于A3C的并行那一套搞了DPPO，我认为其实就是A3C的策略梯度更新的地方的方式改成了PPO的方式。

ACKTR是为了解决策略梯度算法的局限，即参数是按照策略梯度的方向进行更新，但忽略了参数本身的空间结构。因此计算参数的自然梯度，为了避免fisher矩阵计算过慢，因此利用了矩阵分解，它会急剧加快PPO的收敛速度。

连续和离散动作空间/随机和确定策略？
-----------------------------

基于值函数的方法都是利用Q函数（如max Q）来得到策略，因此不适合连续动作空间（DQN等）。而基于策略梯度的方法一般可以通吃。

基于值函数的方法：1）难以应用到随机型策略（stochastic policy）和连续的动作空间。2）value function的微小变化会引起策略变化巨大，从而使训练无法收敛。尤其是引入函数近似（function approximation，FA）后，虽然算法泛化能力提高了，但也引入了bias，从而使得训练的收敛性更加难以保证。

随机和确定策略和上边没什么关系。对于连续动作空间，随机策略常常需要建模分布并采样得到，确定策略则直接输出结果（相当于预测）；而对于离散动作空间，随机策略常用softmax（类似分类）得到概率再采样，确定策略直接argmax。

几项著名工作的主要作用
------------------

1. **trpo**目的是为了找到一种梯度优化方式可以使得目标（reward）单调增长。**ppo**则是为了简化其复杂性，提高效率。
2. **ddpg**给出了确定性策略梯度，并解决了dqn不能解决的（高维）连续动作空间控制问题，但和dqn一样只适合off policy因为利用experience replay来打破数据相关性。而**a3c**则通过多个actor采样的方式解决了数据强相关的问题（训练的时候，同时为多个线程上分配task，学习一遍后，每个线程将自己学习到的参数更新，即异步更新到全局Global Network上，下一次学习的时候拉取全局参数，继续学习），因此适合on policy（在每个worker上）和off policy。
3. 我认为OpenAI系的trpo和ppo是采用优化方法（找到代理优化目标）使得对于policy的更新可以从理论上保证单调趋向最优，从而提高了算法效率，但求解略复杂（ppo更简单）；而deepmind系的dqn和ddpg则是利用了target network和replay buffer来提高采样效率。

理解确定性策略梯度
---------------

[https://zhuanlan.zhihu.com/p/26441204](https://zhuanlan.zhihu.com/p/26441204)中对于随机策略梯度，确定性策略梯度的比较比较明确易懂。随机策略需要大量采样，效率较确定策略低很多。但随机策略中包含了探索和利用，确定性策略对探索的解决方式是off policy+importance sampling。ddpg就是用了神经网络来拟合dpg中的策略和价值函数，本质就是dpg。

**优势函数A=TD Error？**

NO. 看定义式。

**TD(λ)？**

其中的E项的+1表示有另一条trace开始传播，进行更新。schulman证明的gae其实就是把ac的critic用td(λ)。但其实证明的时候是有一项因为无穷所以化简后和td(λ)是同样形式了。
